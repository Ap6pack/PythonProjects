# data_collection.py

import requests
import json

# GitHub API endpoint for search
github_api_search_url = "https://api.github.com/search/repositories"

# Your GitHub personal access token
access_token = "YOUR_ACCESS_TOKEN"

# Keywords related to CVEs and vulnerabilities
search_keywords = "CVE vulnerabilities"

# Define parameters for the search query
params = {
    "q": search_keywords,
    "sort": "stars",  # You can sort by stars, forks, etc.
    "order": "desc",  # Descending order
    "per_page": 10    # Number of results per page
}

# Add your access token to the headers
headers = {
    "Authorization": f"token {access_token}"
}

# Send a GET request to the GitHub API search endpoint
response = requests.get(github_api_search_url, params=params, headers=headers)

# Parse the JSON response
data = response.json()

# Extract repository names and URLs
repositories = []
for item in data.get("items", []):
    repo_name = item["name"]
    repo_url = item["html_url"]
    repositories.append({"name": repo_name, "url": repo_url})

# Save repository data as JSON
with open("scraped_repositories.json", "w") as outfile:
    json.dump(repositories, outfile)

print("Repository data saved to 'scraped_repositories.json'")
